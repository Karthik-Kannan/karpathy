{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8bb3cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7bd8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../makemore/names.txt\") as f:\n",
    "    words = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7531afd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41070c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {v:k for k,v in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c148ad1",
   "metadata": {},
   "source": [
    "## Construct Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de403376",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_dataset(words, block_size=3):  \n",
    "  X, Y = [], []\n",
    "  for w in words:\n",
    "\n",
    "    #print(w)\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      #print(''.join(itos[i] for i in context), '--->', itos[ix])\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe456b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56d38cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# Convert to tensors\n",
    "import random \n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "Xtr , Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xtest, Ytest = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0880e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbfd9b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    generator =  g = torch.Generator().manual_seed(2147483647)\n",
    "    hidden_layer_size: int = 100\n",
    "    lr : float = 0.1\n",
    "    batch_size: int = 32\n",
    "    lookback: int = 3\n",
    "    embedding_size: float = 2\n",
    "    vocab_size: float  =  27"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c3abb2",
   "metadata": {},
   "source": [
    "### Embedded Input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e886e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2571d2d3",
   "metadata": {},
   "source": [
    "### Questions\n",
    "- How do we decide on the size of the embedding?\n",
    "- IS there something unique about language that makes embeddings so good here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea564943",
   "metadata": {},
   "source": [
    "## Create the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16bd8ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self,fan_in, fan_out, bias=True):\n",
    "        self.weight = torch.randn((fan_in, fan_out), generator=Config.generator)\n",
    "        self.bias = torch.zeros(fan_out) if bias else None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out =     x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1055c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want a 2 vector embedding. Start with random embeddings for all 27 chars\n",
    "C = torch.randn(Config.vocab_size ,Config.embedding_size, generator=Config.generator)\n",
    "\n",
    "layers = [ \n",
    "    # the input of this should be 6 because we have 3 previous characters each with 2 dimensional embedding\n",
    "    Linear(Config.embedding_size * Config.lookback, Config.hidden_layer_size), Tanh(),\n",
    "    # Final Layer should output \"probabilities\" for each character\n",
    "    Linear(Config.hidden_layer_size, Config.vocab_size)\n",
    "            ]\n",
    "\n",
    "params = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "\n",
    "for p in params:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bfb8cf",
   "metadata": {},
   "source": [
    "##  Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8d2d998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19e5afcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0, 25],\n",
       "        [ 0, 25, 21],\n",
       "        ...,\n",
       "        [15, 12,  4],\n",
       "        [12,  4,  1],\n",
       "        [ 4,  1, 14]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef1c85b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                 | 0/30000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(13.1755, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(30000)):\n",
    "    # Forward Pass\n",
    "    # Minibatch Construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (Config.batch_size,))\n",
    "    # Go from index input to embedding input\n",
    "    emb = C[Xtr[ix]]\n",
    "    # flatten embedding to multiply with hidden layer\n",
    "    x = emb.view(emb.shape[0], -1 )\n",
    "    # Hidden Layer ouput\n",
    "    for layer in layers:\n",
    "        x =  layer(x)\n",
    "    loss = F.cross_entropy(x, Ytr[ix])\n",
    "    if i%1000 == 0:\n",
    "        print(loss) \n",
    "    \n",
    "    # Backprop\n",
    "    for p in params:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in params:\n",
    "        p.data += -Config.lr * p.grad \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a51f41",
   "metadata": {},
   "source": [
    "### Neural Network Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a4f5a8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[0].bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9900e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc5d4186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 6.1690e-01,  1.5160e+00,  2.4720e-01, -3.7767e-01, -1.9081e+00,\n",
       "          -3.7170e-01, -9.8378e-01, -1.5256e-01, -6.2787e-01,  7.7023e-02,\n",
       "          -1.9911e+00, -1.3050e+00, -1.3792e+00, -3.0560e-01, -5.4209e-01,\n",
       "          -5.9234e-01,  1.0358e+00, -8.6249e-01,  7.8068e-01,  3.8314e-01,\n",
       "           1.4232e+00, -3.6390e-01,  9.4754e-02, -1.1645e+00, -7.2759e-01,\n",
       "           1.1491e+00, -1.1769e+00,  2.6542e-01, -7.1122e-01,  1.0894e+00,\n",
       "          -1.8007e-01,  1.3034e+00, -2.1057e+00, -2.6044e-01,  8.1229e-01,\n",
       "           1.2772e+00,  1.8313e-01,  1.3504e+00,  1.3348e+00,  4.9849e-01,\n",
       "          -5.6533e-01,  5.4281e-01,  1.2600e+00, -6.2020e-01,  1.4325e+00,\n",
       "          -1.0632e+00, -6.0596e-01,  9.1776e-01,  9.0187e-01,  1.2028e+00,\n",
       "          -3.7941e-01, -9.8748e-01, -1.1833e+00,  1.5222e+00,  4.3905e-01,\n",
       "           1.6933e-01, -1.4777e+00, -5.1029e-01, -1.7485e+00,  6.7062e-01,\n",
       "           3.3509e-01, -5.3509e-01, -1.1953e+00, -7.0163e-01,  3.8761e-01,\n",
       "           4.7236e-01, -3.3502e-01,  2.5192e-01,  8.0882e-01, -7.5013e-01,\n",
       "           8.0529e-01, -1.2711e+00,  6.1662e-01, -6.2197e-01, -5.6079e-01,\n",
       "          -2.0003e-01,  2.1521e-01,  2.5937e-01,  7.6018e-01,  1.7334e-01,\n",
       "           5.5570e-01,  4.7458e-01,  1.4484e+00, -5.4724e-01,  3.5970e-01,\n",
       "          -8.0573e-02,  7.5720e-01,  7.2333e-01, -1.9062e+00, -4.2753e-01,\n",
       "          -1.0877e+00, -1.8156e-02, -2.8800e-01,  8.7248e-01, -5.5840e-02,\n",
       "           5.0983e-01,  7.0311e-01,  4.0716e-01, -8.2984e-01,  2.1452e+00],\n",
       "         [ 3.3263e-01, -2.6167e-01, -2.1089e-01, -7.5038e-01, -2.8971e-01,\n",
       "          -3.1566e-01,  8.6907e-01,  7.7798e-02, -4.6144e-01,  6.0982e-01,\n",
       "          -2.5358e-01, -6.3923e-01, -1.2464e+00,  6.8305e-01, -1.1567e+00,\n",
       "           1.9772e-01, -3.0979e-01, -6.9539e-01, -3.6946e-01, -1.2707e+00,\n",
       "          -2.1286e-01,  5.0950e-01, -5.8031e-01, -6.3805e-02,  3.6892e-02,\n",
       "          -2.5443e-01, -1.2175e-01, -7.9235e-01, -4.5285e-01, -4.9578e-01,\n",
       "          -3.5451e-01, -2.4350e-01, -7.8078e-01, -5.9604e-01, -1.4941e+00,\n",
       "          -5.9933e-01,  8.1415e-01, -7.3806e-01,  5.8741e-01, -3.9421e-01,\n",
       "          -2.4034e-01,  7.2719e-01, -9.1503e-01, -8.2739e-01, -4.8838e-01,\n",
       "          -7.3712e-01, -2.1282e-02, -1.1940e+00, -5.3429e-01, -2.6644e-01,\n",
       "           2.6029e-01,  9.0901e-01, -8.3952e-01,  1.9098e-01,  5.5367e-01,\n",
       "          -2.1494e-01, -6.1300e-01, -2.1743e-01, -7.3050e-01, -1.0926e+00,\n",
       "          -1.0527e+00, -1.4367e-01, -1.2712e-01, -2.9877e-01,  9.5274e-01,\n",
       "           4.0218e-01, -8.1571e-01,  6.4045e-01, -3.7846e-01,  7.9577e-03,\n",
       "           1.2728e+00,  7.9192e-01, -9.0431e-01,  4.3201e-02, -7.2261e-01,\n",
       "           5.2054e-01,  2.0710e-01,  1.0033e+00, -7.5487e-01, -1.9403e+00,\n",
       "           4.4959e-01, -1.0094e+00, -1.4627e-01, -5.2541e-01, -4.0091e-01,\n",
       "           5.6533e-01, -1.8436e+00, -1.1698e+00, -1.2906e+00,  2.4456e-01,\n",
       "           5.4904e-02,  1.3537e+00,  1.3423e+00,  1.5184e-01, -2.4990e-01,\n",
       "          -6.3755e-02,  1.5511e-02, -1.7323e+00, -3.0010e+00,  1.4111e+00],\n",
       "         [ 1.6113e-01,  4.4925e-01,  1.1059e+00,  2.7318e-01,  1.3391e+00,\n",
       "          -1.9907e+00, -5.6008e-01,  8.1045e-01, -1.5047e+00,  8.4682e-01,\n",
       "           1.3843e+00,  3.5017e-01, -1.3424e+00,  4.1706e-01, -1.3601e+00,\n",
       "          -8.1312e-01, -8.5259e-01,  1.0464e+00, -2.7178e+00, -1.0918e+00,\n",
       "           6.1121e-01, -8.0651e-01, -1.8109e-01,  2.9729e-01, -9.4046e-01,\n",
       "          -1.2773e-01, -7.5068e-01,  1.0315e+00, -3.4760e-01, -6.3099e-01,\n",
       "           5.2696e-02, -1.2992e-01, -9.1355e-01,  7.1023e-01,  9.5530e-01,\n",
       "           1.5930e+00,  1.0226e+00, -1.6860e+00,  1.6305e+00, -5.9521e-01,\n",
       "          -4.0636e-01, -7.4648e-01,  3.9051e-01,  2.3522e-01,  1.2821e+00,\n",
       "           5.8325e-02, -1.6096e-01,  3.6318e-01, -6.1577e-01,  7.0467e-01,\n",
       "           1.8443e-01,  8.5019e-02, -3.1827e-01,  1.1445e+00,  2.7964e-01,\n",
       "           1.6638e-01,  8.3416e-01, -1.0505e+00,  1.3007e-01,  2.1444e-01,\n",
       "          -7.6467e-01, -6.8719e-01,  8.5603e-01, -5.8552e-01, -1.3270e+00,\n",
       "          -8.2526e-01,  6.5662e-01, -3.7992e-01,  9.6231e-01, -6.7906e-01,\n",
       "          -4.1538e-01, -1.4429e+00, -5.6192e-01, -3.0423e-01, -3.7793e-01,\n",
       "          -5.3766e-01, -4.6057e-01, -6.7402e-01,  2.3807e-01,  2.4068e-01,\n",
       "          -1.4790e-01,  1.9359e-01,  4.6118e-02, -4.9368e-01,  4.4903e-02,\n",
       "           2.9965e-03,  4.7139e-02,  3.0955e-01, -3.0404e-02,  1.5622e+00,\n",
       "          -1.6070e+00, -1.8674e+00,  6.5678e-01,  2.0970e+00,  6.3649e-01,\n",
       "          -5.9636e-01, -8.2709e-01,  8.2253e-01,  1.7068e+00,  6.7033e-02],\n",
       "         [-2.2439e-01,  5.1658e-01, -9.4188e-01,  1.5729e+00, -2.3348e-01,\n",
       "           1.5447e+00, -4.7348e-01, -2.3107e-01,  1.7249e-01,  1.9007e-03,\n",
       "          -9.5376e-02, -9.2531e-01,  4.0858e-01, -1.9692e+00,  8.7924e-01,\n",
       "           2.1157e+00, -9.1996e-01,  1.3770e+00,  9.3221e-01,  3.6407e-01,\n",
       "          -6.3108e-01,  1.3431e+00, -1.1013e+00, -1.5959e+00, -7.8810e-01,\n",
       "          -1.6357e-01,  1.0800e+00, -3.1465e-01, -7.6019e-01,  9.0109e-03,\n",
       "           3.8267e-01, -9.3139e-01,  1.2952e-01, -2.0498e+00, -6.4858e-01,\n",
       "           6.9206e-01,  1.2910e+00, -1.5094e+00,  9.4376e-01, -7.9048e-02,\n",
       "           5.1374e-01,  4.6094e-01,  3.4197e-01, -3.5208e-01, -7.6635e-01,\n",
       "          -2.1007e+00, -2.5436e-01,  3.4370e-01,  1.4041e+00,  8.9889e-01,\n",
       "          -7.7537e-01,  1.2605e+00, -6.2477e-01, -1.0825e-01,  1.7588e+00,\n",
       "           4.8435e-03,  6.4799e-01, -7.9564e-01,  1.3792e+00,  4.2930e-01,\n",
       "          -2.4174e-01,  1.5490e+00,  6.8348e-01,  3.3164e-01,  2.7701e-02,\n",
       "           1.3175e+00,  6.5369e-01,  1.7147e+00,  2.0656e-02,  7.1300e-01,\n",
       "          -1.1804e+00,  1.4475e+00, -5.6838e-01, -2.0969e+00,  2.6144e-02,\n",
       "          -1.1874e+00, -2.5424e+00,  2.5033e-01, -2.7404e-01, -1.4597e+00,\n",
       "          -3.7249e-01, -9.2493e-01,  2.0206e+00,  3.5374e-01, -1.0102e+00,\n",
       "          -1.0522e+00, -7.1766e-01, -7.8781e-01, -6.6889e-01,  8.7467e-02,\n",
       "           8.5424e-01,  1.7877e-01, -2.2316e+00, -1.1603e+00, -1.5095e-01,\n",
       "          -8.7039e-01, -1.2377e+00, -9.0766e-01, -4.5240e-01, -9.8853e-01],\n",
       "         [-7.0682e-01,  9.5190e-01,  3.9723e-02, -1.5372e+00, -9.7812e-01,\n",
       "           3.2481e-01,  6.4204e-01, -7.8512e-01,  4.4456e-01, -7.1861e-01,\n",
       "           1.5383e+00,  1.0279e+00,  1.1145e-02,  6.4026e-01, -8.6427e-01,\n",
       "          -1.7950e-01,  1.8765e-01,  7.5974e-01, -1.1187e+00,  1.1622e+00,\n",
       "          -2.2014e-01,  6.1003e-01,  3.1496e+00, -1.4986e+00,  1.3419e+00,\n",
       "           1.1878e+00, -6.1435e-01, -7.8807e-01,  3.6980e-01, -9.8893e-01,\n",
       "          -4.8677e-01,  8.2455e-01,  3.8262e-02,  3.9877e-02, -1.9103e-01,\n",
       "           2.7412e-01,  4.6849e-01, -1.9482e+00,  1.2960e+00,  1.2675e+00,\n",
       "          -1.2273e+00, -4.1746e-01, -1.3395e+00,  1.1077e+00,  2.0181e+00,\n",
       "           2.3471e-01, -9.9145e-01, -1.9684e+00,  1.1179e+00, -6.0053e-01,\n",
       "           2.2064e+00, -1.1409e+00, -9.1581e-01,  1.8654e+00,  1.9563e+00,\n",
       "           8.0614e-01,  4.2016e-01, -6.8729e-01,  8.8593e-01,  6.1130e-01,\n",
       "           2.3302e-01, -3.1072e-01, -6.5239e-02, -2.3217e-01,  1.0733e+00,\n",
       "          -5.7453e-01,  2.1403e-01,  2.1316e-01,  5.5784e-01,  7.1076e-01,\n",
       "           4.2613e-01,  1.3364e-01, -2.3912e-01,  8.7498e-02, -1.1193e+00,\n",
       "           4.6231e-01,  8.6182e-01, -2.8891e-01, -7.9521e-01,  1.8388e+00,\n",
       "           6.4083e-01,  1.6075e+00, -1.7244e+00,  9.5785e-01,  9.1433e-01,\n",
       "          -2.4027e-01, -8.5465e-01,  1.9764e-01,  2.6952e-01,  1.1835e+00,\n",
       "          -1.3968e+00, -4.5004e-01, -8.3706e-01,  3.6308e-01, -1.6085e-01,\n",
       "          -9.9655e-02, -4.0503e-01,  4.5696e-01, -2.3215e+00, -1.0990e+00],\n",
       "         [ 5.2891e-01, -2.2218e+00, -5.2063e-01, -5.5786e-01, -4.7729e-02,\n",
       "          -4.4920e-01,  2.9185e-01,  1.2820e+00, -4.4728e-01, -6.2013e-02,\n",
       "           2.1940e+00,  6.6979e-01,  3.5252e-01, -5.2400e-01,  5.7828e-01,\n",
       "          -7.1595e-01, -7.8246e-01, -1.0697e+00, -1.4596e-01, -1.2915e+00,\n",
       "           1.0465e-01, -4.6464e-01, -2.1036e+00, -1.4391e+00,  5.0585e-01,\n",
       "          -4.8995e-01, -1.8563e+00,  9.6090e-01, -3.3916e-01, -1.8058e+00,\n",
       "          -3.3235e-01,  2.0767e+00, -1.0325e+00,  6.9194e-01,  2.9856e-01,\n",
       "           7.9499e-01, -7.5348e-01,  1.7128e+00,  5.9542e-02,  1.1308e+00,\n",
       "           1.6935e+00,  4.1456e-01, -2.0954e+00, -7.0722e-01, -1.0279e+00,\n",
       "          -3.9554e-01, -9.6075e-01, -2.4620e+00, -1.7341e+00, -1.0360e+00,\n",
       "          -1.3371e+00, -4.0491e-01, -2.3470e-01, -1.3765e+00,  9.7698e-01,\n",
       "           8.1994e-01, -1.6190e+00,  2.0390e+00, -4.5616e-01, -5.4802e-01,\n",
       "           2.3267e-01, -3.9205e-01, -4.6894e-01,  1.2568e-01,  1.5859e-01,\n",
       "           1.3777e+00,  1.1426e+00, -2.1952e-01, -6.0779e-01, -1.1493e-01,\n",
       "           2.4764e+00, -9.9009e-02,  7.0165e-01,  1.1718e+00, -1.6913e+00,\n",
       "          -9.0056e-01,  4.5111e-01,  6.4420e-01,  1.0047e+00,  3.3600e-01,\n",
       "          -1.4115e+00, -3.6833e-01,  1.9901e-01, -2.7436e-01,  6.1649e-01,\n",
       "           8.0931e-01, -2.6301e-01, -7.5521e-01,  8.1911e-01,  7.4140e-01,\n",
       "          -5.8787e-01, -4.6505e-01,  9.4403e-01, -4.8031e-01, -3.5158e-01,\n",
       "           3.6381e-01,  2.5769e+00,  1.4544e+00, -6.1003e-01, -5.9961e-01]],\n",
       "        requires_grad=True),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.], requires_grad=True)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers[0].parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60cd5eb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fastbook/lib/python3.10/site-packages/pandas/core/frame.py:761\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    753\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    754\u001b[0m             arrays,\n\u001b[1;32m    755\u001b[0m             columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    758\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    759\u001b[0m         )\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 761\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    770\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    771\u001b[0m         {},\n\u001b[1;32m    772\u001b[0m         index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    775\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    776\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/fastbook/lib/python3.10/site-packages/pandas/core/internals/construction.py:329\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    324\u001b[0m         values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_prep_ndarraylike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_on_sanitize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_equal(values\u001b[38;5;241m.\u001b[39mdtype, dtype):\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;66;03m# GH#40110 see similar check inside sanitize_array\u001b[39;00m\n\u001b[1;32m    333\u001b[0m     rcf \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m (is_integer_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fastbook/lib/python3.10/site-packages/pandas/core/internals/construction.py:568\u001b[0m, in \u001b[0;36m_prep_ndarraylike\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# we could have a 1-dim or 2-dim list here\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# this is equiv of np.asarray, but does object conversion\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# and platform dtype preservation\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(values[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m--> 568\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m values[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# GH#21861 see test_constructor_list_of_lists\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([convert(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/miniconda3/envs/fastbook/lib/python3.10/site-packages/torch/_tensor.py:955\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "pd.DataFrame(layers[0].parameters(), columns=[\"weights\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e7d6ebe",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m alt\u001b[38;5;241m.\u001b[39mChart(data\u001b[38;5;241m=\u001b[39m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmark_bar()\n",
      "File \u001b[0;32m~/miniconda3/envs/fastbook/lib/python3.10/site-packages/pandas/core/frame.py:761\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    753\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m arrays_to_mgr(\n\u001b[1;32m    754\u001b[0m             arrays,\n\u001b[1;32m    755\u001b[0m             columns,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    758\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    759\u001b[0m         )\n\u001b[1;32m    760\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 761\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m \u001b[43mndarray_to_mgr\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    770\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[1;32m    771\u001b[0m         {},\n\u001b[1;32m    772\u001b[0m         index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    775\u001b[0m         typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[1;32m    776\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/fastbook/lib/python3.10/site-packages/pandas/core/internals/construction.py:329\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[0;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[1;32m    324\u001b[0m         values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# by definition an array here\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;66;03m# the dtypes will be coerced to a single dtype\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_prep_ndarraylike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy_on_sanitize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_equal(values\u001b[38;5;241m.\u001b[39mdtype, dtype):\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;66;03m# GH#40110 see similar check inside sanitize_array\u001b[39;00m\n\u001b[1;32m    333\u001b[0m     rcf \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m (is_integer_dtype(dtype) \u001b[38;5;129;01mand\u001b[39;00m values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fastbook/lib/python3.10/site-packages/pandas/core/internals/construction.py:568\u001b[0m, in \u001b[0;36m_prep_ndarraylike\u001b[0;34m(values, copy)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# we could have a 1-dim or 2-dim list here\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;66;03m# this is equiv of np.asarray, but does object conversion\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# and platform dtype preservation\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(values[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m--> 568\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values[\u001b[38;5;241m0\u001b[39m], np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m values[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;66;03m# GH#21861 see test_constructor_list_of_lists\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([convert(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m values])\n",
      "File \u001b[0;32m~/miniconda3/envs/fastbook/lib/python3.10/site-packages/torch/_tensor.py:955\u001b[0m, in \u001b[0;36mTensor.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    953\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m__array__, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "alt.Chart(data=pd.DataFrame(layers[0].parameters()), columns=\"weights\").mark_bar().encode(x{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6896bd7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 6.1690e-01,  1.5160e+00,  2.4720e-01, -3.7767e-01, -1.9081e+00,\n",
       "          -3.7170e-01, -9.8378e-01, -1.5256e-01, -6.2787e-01,  7.7023e-02,\n",
       "          -1.9911e+00, -1.3050e+00, -1.3792e+00, -3.0560e-01, -5.4209e-01,\n",
       "          -5.9234e-01,  1.0358e+00, -8.6249e-01,  7.8068e-01,  3.8314e-01,\n",
       "           1.4232e+00, -3.6390e-01,  9.4754e-02, -1.1645e+00, -7.2759e-01,\n",
       "           1.1491e+00, -1.1769e+00,  2.6542e-01, -7.1122e-01,  1.0894e+00,\n",
       "          -1.8007e-01,  1.3034e+00, -2.1057e+00, -2.6044e-01,  8.1229e-01,\n",
       "           1.2772e+00,  1.8313e-01,  1.3504e+00,  1.3348e+00,  4.9849e-01,\n",
       "          -5.6533e-01,  5.4281e-01,  1.2600e+00, -6.2020e-01,  1.4325e+00,\n",
       "          -1.0632e+00, -6.0596e-01,  9.1776e-01,  9.0187e-01,  1.2028e+00,\n",
       "          -3.7941e-01, -9.8748e-01, -1.1833e+00,  1.5222e+00,  4.3905e-01,\n",
       "           1.6933e-01, -1.4777e+00, -5.1029e-01, -1.7485e+00,  6.7062e-01,\n",
       "           3.3509e-01, -5.3509e-01, -1.1953e+00, -7.0163e-01,  3.8761e-01,\n",
       "           4.7236e-01, -3.3502e-01,  2.5192e-01,  8.0882e-01, -7.5013e-01,\n",
       "           8.0529e-01, -1.2711e+00,  6.1662e-01, -6.2197e-01, -5.6079e-01,\n",
       "          -2.0003e-01,  2.1521e-01,  2.5937e-01,  7.6018e-01,  1.7334e-01,\n",
       "           5.5570e-01,  4.7458e-01,  1.4484e+00, -5.4724e-01,  3.5970e-01,\n",
       "          -8.0573e-02,  7.5720e-01,  7.2333e-01, -1.9062e+00, -4.2753e-01,\n",
       "          -1.0877e+00, -1.8156e-02, -2.8800e-01,  8.7248e-01, -5.5840e-02,\n",
       "           5.0983e-01,  7.0311e-01,  4.0716e-01, -8.2984e-01,  2.1452e+00],\n",
       "         [ 3.3263e-01, -2.6167e-01, -2.1089e-01, -7.5038e-01, -2.8971e-01,\n",
       "          -3.1566e-01,  8.6907e-01,  7.7798e-02, -4.6144e-01,  6.0982e-01,\n",
       "          -2.5358e-01, -6.3923e-01, -1.2464e+00,  6.8305e-01, -1.1567e+00,\n",
       "           1.9772e-01, -3.0979e-01, -6.9539e-01, -3.6946e-01, -1.2707e+00,\n",
       "          -2.1286e-01,  5.0950e-01, -5.8031e-01, -6.3805e-02,  3.6892e-02,\n",
       "          -2.5443e-01, -1.2175e-01, -7.9235e-01, -4.5285e-01, -4.9578e-01,\n",
       "          -3.5451e-01, -2.4350e-01, -7.8078e-01, -5.9604e-01, -1.4941e+00,\n",
       "          -5.9933e-01,  8.1415e-01, -7.3806e-01,  5.8741e-01, -3.9421e-01,\n",
       "          -2.4034e-01,  7.2719e-01, -9.1503e-01, -8.2739e-01, -4.8838e-01,\n",
       "          -7.3712e-01, -2.1282e-02, -1.1940e+00, -5.3429e-01, -2.6644e-01,\n",
       "           2.6029e-01,  9.0901e-01, -8.3952e-01,  1.9098e-01,  5.5367e-01,\n",
       "          -2.1494e-01, -6.1300e-01, -2.1743e-01, -7.3050e-01, -1.0926e+00,\n",
       "          -1.0527e+00, -1.4367e-01, -1.2712e-01, -2.9877e-01,  9.5274e-01,\n",
       "           4.0218e-01, -8.1571e-01,  6.4045e-01, -3.7846e-01,  7.9577e-03,\n",
       "           1.2728e+00,  7.9192e-01, -9.0431e-01,  4.3201e-02, -7.2261e-01,\n",
       "           5.2054e-01,  2.0710e-01,  1.0033e+00, -7.5487e-01, -1.9403e+00,\n",
       "           4.4959e-01, -1.0094e+00, -1.4627e-01, -5.2541e-01, -4.0091e-01,\n",
       "           5.6533e-01, -1.8436e+00, -1.1698e+00, -1.2906e+00,  2.4456e-01,\n",
       "           5.4904e-02,  1.3537e+00,  1.3423e+00,  1.5184e-01, -2.4990e-01,\n",
       "          -6.3755e-02,  1.5511e-02, -1.7323e+00, -3.0010e+00,  1.4111e+00],\n",
       "         [ 1.6113e-01,  4.4925e-01,  1.1059e+00,  2.7318e-01,  1.3391e+00,\n",
       "          -1.9907e+00, -5.6008e-01,  8.1045e-01, -1.5047e+00,  8.4682e-01,\n",
       "           1.3843e+00,  3.5017e-01, -1.3424e+00,  4.1706e-01, -1.3601e+00,\n",
       "          -8.1312e-01, -8.5259e-01,  1.0464e+00, -2.7178e+00, -1.0918e+00,\n",
       "           6.1121e-01, -8.0651e-01, -1.8109e-01,  2.9729e-01, -9.4046e-01,\n",
       "          -1.2773e-01, -7.5068e-01,  1.0315e+00, -3.4760e-01, -6.3099e-01,\n",
       "           5.2696e-02, -1.2992e-01, -9.1355e-01,  7.1023e-01,  9.5530e-01,\n",
       "           1.5930e+00,  1.0226e+00, -1.6860e+00,  1.6305e+00, -5.9521e-01,\n",
       "          -4.0636e-01, -7.4648e-01,  3.9051e-01,  2.3522e-01,  1.2821e+00,\n",
       "           5.8325e-02, -1.6096e-01,  3.6318e-01, -6.1577e-01,  7.0467e-01,\n",
       "           1.8443e-01,  8.5019e-02, -3.1827e-01,  1.1445e+00,  2.7964e-01,\n",
       "           1.6638e-01,  8.3416e-01, -1.0505e+00,  1.3007e-01,  2.1444e-01,\n",
       "          -7.6467e-01, -6.8719e-01,  8.5603e-01, -5.8552e-01, -1.3270e+00,\n",
       "          -8.2526e-01,  6.5662e-01, -3.7992e-01,  9.6231e-01, -6.7906e-01,\n",
       "          -4.1538e-01, -1.4429e+00, -5.6192e-01, -3.0423e-01, -3.7793e-01,\n",
       "          -5.3766e-01, -4.6057e-01, -6.7402e-01,  2.3807e-01,  2.4068e-01,\n",
       "          -1.4790e-01,  1.9359e-01,  4.6118e-02, -4.9368e-01,  4.4903e-02,\n",
       "           2.9965e-03,  4.7139e-02,  3.0955e-01, -3.0404e-02,  1.5622e+00,\n",
       "          -1.6070e+00, -1.8674e+00,  6.5678e-01,  2.0970e+00,  6.3649e-01,\n",
       "          -5.9636e-01, -8.2709e-01,  8.2253e-01,  1.7068e+00,  6.7033e-02],\n",
       "         [-2.2439e-01,  5.1658e-01, -9.4188e-01,  1.5729e+00, -2.3348e-01,\n",
       "           1.5447e+00, -4.7348e-01, -2.3107e-01,  1.7249e-01,  1.9007e-03,\n",
       "          -9.5376e-02, -9.2531e-01,  4.0858e-01, -1.9692e+00,  8.7924e-01,\n",
       "           2.1157e+00, -9.1996e-01,  1.3770e+00,  9.3221e-01,  3.6407e-01,\n",
       "          -6.3108e-01,  1.3431e+00, -1.1013e+00, -1.5959e+00, -7.8810e-01,\n",
       "          -1.6357e-01,  1.0800e+00, -3.1465e-01, -7.6019e-01,  9.0109e-03,\n",
       "           3.8267e-01, -9.3139e-01,  1.2952e-01, -2.0498e+00, -6.4858e-01,\n",
       "           6.9206e-01,  1.2910e+00, -1.5094e+00,  9.4376e-01, -7.9048e-02,\n",
       "           5.1374e-01,  4.6094e-01,  3.4197e-01, -3.5208e-01, -7.6635e-01,\n",
       "          -2.1007e+00, -2.5436e-01,  3.4370e-01,  1.4041e+00,  8.9889e-01,\n",
       "          -7.7537e-01,  1.2605e+00, -6.2477e-01, -1.0825e-01,  1.7588e+00,\n",
       "           4.8435e-03,  6.4799e-01, -7.9564e-01,  1.3792e+00,  4.2930e-01,\n",
       "          -2.4174e-01,  1.5490e+00,  6.8348e-01,  3.3164e-01,  2.7701e-02,\n",
       "           1.3175e+00,  6.5369e-01,  1.7147e+00,  2.0656e-02,  7.1300e-01,\n",
       "          -1.1804e+00,  1.4475e+00, -5.6838e-01, -2.0969e+00,  2.6144e-02,\n",
       "          -1.1874e+00, -2.5424e+00,  2.5033e-01, -2.7404e-01, -1.4597e+00,\n",
       "          -3.7249e-01, -9.2493e-01,  2.0206e+00,  3.5374e-01, -1.0102e+00,\n",
       "          -1.0522e+00, -7.1766e-01, -7.8781e-01, -6.6889e-01,  8.7467e-02,\n",
       "           8.5424e-01,  1.7877e-01, -2.2316e+00, -1.1603e+00, -1.5095e-01,\n",
       "          -8.7039e-01, -1.2377e+00, -9.0766e-01, -4.5240e-01, -9.8853e-01],\n",
       "         [-7.0682e-01,  9.5190e-01,  3.9723e-02, -1.5372e+00, -9.7812e-01,\n",
       "           3.2481e-01,  6.4204e-01, -7.8512e-01,  4.4456e-01, -7.1861e-01,\n",
       "           1.5383e+00,  1.0279e+00,  1.1145e-02,  6.4026e-01, -8.6427e-01,\n",
       "          -1.7950e-01,  1.8765e-01,  7.5974e-01, -1.1187e+00,  1.1622e+00,\n",
       "          -2.2014e-01,  6.1003e-01,  3.1496e+00, -1.4986e+00,  1.3419e+00,\n",
       "           1.1878e+00, -6.1435e-01, -7.8807e-01,  3.6980e-01, -9.8893e-01,\n",
       "          -4.8677e-01,  8.2455e-01,  3.8262e-02,  3.9877e-02, -1.9103e-01,\n",
       "           2.7412e-01,  4.6849e-01, -1.9482e+00,  1.2960e+00,  1.2675e+00,\n",
       "          -1.2273e+00, -4.1746e-01, -1.3395e+00,  1.1077e+00,  2.0181e+00,\n",
       "           2.3471e-01, -9.9145e-01, -1.9684e+00,  1.1179e+00, -6.0053e-01,\n",
       "           2.2064e+00, -1.1409e+00, -9.1581e-01,  1.8654e+00,  1.9563e+00,\n",
       "           8.0614e-01,  4.2016e-01, -6.8729e-01,  8.8593e-01,  6.1130e-01,\n",
       "           2.3302e-01, -3.1072e-01, -6.5239e-02, -2.3217e-01,  1.0733e+00,\n",
       "          -5.7453e-01,  2.1403e-01,  2.1316e-01,  5.5784e-01,  7.1076e-01,\n",
       "           4.2613e-01,  1.3364e-01, -2.3912e-01,  8.7498e-02, -1.1193e+00,\n",
       "           4.6231e-01,  8.6182e-01, -2.8891e-01, -7.9521e-01,  1.8388e+00,\n",
       "           6.4083e-01,  1.6075e+00, -1.7244e+00,  9.5785e-01,  9.1433e-01,\n",
       "          -2.4027e-01, -8.5465e-01,  1.9764e-01,  2.6952e-01,  1.1835e+00,\n",
       "          -1.3968e+00, -4.5004e-01, -8.3706e-01,  3.6308e-01, -1.6085e-01,\n",
       "          -9.9655e-02, -4.0503e-01,  4.5696e-01, -2.3215e+00, -1.0990e+00],\n",
       "         [ 5.2891e-01, -2.2218e+00, -5.2063e-01, -5.5786e-01, -4.7729e-02,\n",
       "          -4.4920e-01,  2.9185e-01,  1.2820e+00, -4.4728e-01, -6.2013e-02,\n",
       "           2.1940e+00,  6.6979e-01,  3.5252e-01, -5.2400e-01,  5.7828e-01,\n",
       "          -7.1595e-01, -7.8246e-01, -1.0697e+00, -1.4596e-01, -1.2915e+00,\n",
       "           1.0465e-01, -4.6464e-01, -2.1036e+00, -1.4391e+00,  5.0585e-01,\n",
       "          -4.8995e-01, -1.8563e+00,  9.6090e-01, -3.3916e-01, -1.8058e+00,\n",
       "          -3.3235e-01,  2.0767e+00, -1.0325e+00,  6.9194e-01,  2.9856e-01,\n",
       "           7.9499e-01, -7.5348e-01,  1.7128e+00,  5.9542e-02,  1.1308e+00,\n",
       "           1.6935e+00,  4.1456e-01, -2.0954e+00, -7.0722e-01, -1.0279e+00,\n",
       "          -3.9554e-01, -9.6075e-01, -2.4620e+00, -1.7341e+00, -1.0360e+00,\n",
       "          -1.3371e+00, -4.0491e-01, -2.3470e-01, -1.3765e+00,  9.7698e-01,\n",
       "           8.1994e-01, -1.6190e+00,  2.0390e+00, -4.5616e-01, -5.4802e-01,\n",
       "           2.3267e-01, -3.9205e-01, -4.6894e-01,  1.2568e-01,  1.5859e-01,\n",
       "           1.3777e+00,  1.1426e+00, -2.1952e-01, -6.0779e-01, -1.1493e-01,\n",
       "           2.4764e+00, -9.9009e-02,  7.0165e-01,  1.1718e+00, -1.6913e+00,\n",
       "          -9.0056e-01,  4.5111e-01,  6.4420e-01,  1.0047e+00,  3.3600e-01,\n",
       "          -1.4115e+00, -3.6833e-01,  1.9901e-01, -2.7436e-01,  6.1649e-01,\n",
       "           8.0931e-01, -2.6301e-01, -7.5521e-01,  8.1911e-01,  7.4140e-01,\n",
       "          -5.8787e-01, -4.6505e-01,  9.4403e-01, -4.8031e-01, -3.5158e-01,\n",
       "           3.6381e-01,  2.5769e+00,  1.4544e+00, -6.1003e-01, -5.9961e-01]],\n",
       "        requires_grad=True),\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.], requires_grad=True)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charts = []\n",
    "alt.layers[0].parameters()\n",
    "# for layer in layers[:-1]:\n",
    "#     if type(layer) == Tanh:\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e919796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3740, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "emb = C[Xdev]\n",
    "# flatten embedding to multiply with hidden layer\n",
    "x = emb.view(-1, 6)\n",
    "# Hidden Layer ouput\n",
    "for layer in layers:\n",
    "    x = layer(x)\n",
    "# Cross Entropy\n",
    "loss = F.cross_entropy(x, Ydev)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a94ca",
   "metadata": {},
   "source": [
    "## Questions\n",
    "- Is how we tokenize also a hyperaparameter?\n",
    "- Is the subsitution in language a necessary condition for embeddings to work?\n",
    "- Is the loss a good enough rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c874625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "576bf75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 5\n",
    "block_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21cdec16",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 1) (544193499.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [19], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    '\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 1)\n"
     ]
    }
   ],
   "source": [
    "'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabb6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [0] *3\n",
    "emb = C[context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ea7b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93726cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_words):\n",
    "    context = [0] * block_size\n",
    "    word = \"\"\n",
    "    ix = -1\n",
    "    while ix != 0:\n",
    "        emb = C[context]\n",
    "        # flatten embedding to multiply with hidden layer\n",
    "        flatemb = emb.view(-1, lookback * embedding_size)\n",
    "        # Hidden Layer ouput\n",
    "        h = torch.tanh(flatemb @ w1 + b1)\n",
    "        # Logits\n",
    "        h = h @ w2 + b2\n",
    "        probs = torch.softmax(h, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "        word += itos[ix]\n",
    "        context = context[1:] + [ix]\n",
    "    print (word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6589e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastbook] *",
   "language": "python",
   "name": "conda-env-fastbook-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
